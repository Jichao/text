#!/usr/bin/env python
# -*- coding: utf-8 -*-

from generate_unicode_normalization_data import cccs
from generate_unicode_normalization_data import expand_decomp_canonical
from generate_unicode_normalization_data import get_decompositions

weights_header_form = '''\
// Warning! This file is autogenerated.
#ifndef BOOST_TEXT_COLLATION_WEIGHTS_HPP
#define BOOST_TEXT_COLLATION_WEIGHTS_HPP


namespace boost {{ namespace text {{

/** TODO
    Min values exclude 0. */
enum class collation_weights : int {{
    min_l1 = {0:>6},        /// The minumum nonzero L1 collation weight.
    max_l1 = {1:>6},        /// The maxumum nonzero L1 collation weight.

    min_l2 = {2:>6},        /// The minumum nonzero L2 collation weight.
    max_l2 = {3:>6},        /// The maxumum nonzero L2 collation weight.

    min_l3 = {4:>6},        /// The minumum nonzero L3 collation weight.
    max_l3 = {5:>6},        /// The maxumum nonzero L3 collation weight.

    min_variable = {6:>6},  /// The minumum variable L1 collation weight.
    max_variable = {7:>6}   /// The maxumum variable L1 collation weight.
}};

}}}}

#endif
'''

file_form = '''\
// Warning! This file is autogenerated.
#include <boost/text/collation_data.hpp>
#include <boost/text/collation_weights.hpp>

#include <unordered_map>


namespace boost {{ namespace text {{

// For the compression schemes used elsewhere in sort key generation to work,
// these must be well-formed.  See:
// http://www.unicode.org/reports/tr10/#Eliminating_level_separators
// http://www.unicode.org/reports/tr10/#L2/L3_in_8_bits

static_assert(
    static_cast<int>(collation_weights::max_l2) < static_cast<int>(collation_weights::min_l1),
    "Oops!  The max L2 collation weight must be less than the min L1 weight.");
static_assert(
    static_cast<int>(collation_weights::max_l3) < static_cast<int>(collation_weights::min_l2),
    "Oops!  The max L3 collation weight must be less than the min L2 weight.");
static_assert(
    static_cast<int>(collation_weights::max_l2) - static_cast<int>(collation_weights::min_l2) < 256,
    "Oops!  The range of L2 collation weights must be < 256.");

static const std::unordered_map<code_points<4>, collation_elements> g_collation_element_table = {{
{0}
}};

namespace detail {{
    collation_elements collation_impl(code_points<4> cps) noexcept
    {{
        auto const it = g_collation_element_table.find(cps);
        if (it == g_collation_element_table.end())
            return collation_elements{{{{}}, 0}};
        return it->second;
    }}
}}

}}}}
'''


def get_ducet(filename):
    ducet = {}

    min_var = 10000
    max_var = 0
    min_non_var = 10000
    max_non_var = 0
    min_l2 = 10000
    max_l2 = 0
    min_l3 = 10000
    max_l3 = 0
    lines = open(filename, 'r').readlines()
    for line in lines:
        line = line[:-1]
        if not line.startswith('#') and not line.startswith('@') and len(line) != 0:
            comment_start = line.find('#')
            comment = ''
            if comment_start != -1:
                comment = line[comment_start + 1:].strip()
                line = line[:comment_start]
            fields = map(lambda x: x.strip(), line.split(';'))
            cps = tuple(map(lambda x: int(x, 16), fields[0].split(' ')))
            collation_elements = fields[1][1:-1].split('][')
            collation_elements = map(
                lambda x: (
                    map(lambda y: int(y, 16), x[1:].split('.')),
                    x[0] == '*'
                ),
                collation_elements
            )
            for e in collation_elements:
                if e[1]:
                    min_var = min(min_var, e[0][0])
                    max_var = max(max_var, e[0][0])
                else:
                    if e[0][0] != 0:
                        min_non_var = min(min_non_var, e[0][0])
                    max_non_var = max(max_non_var, e[0][0])
                if e[0][1] != 0:
                    min_l2 = min(min_l2, e[0][1])
                max_l2 = max(max_l2, e[0][1])
                if e[0][2] != 0:
                    min_l3 = min(min_l3, e[0][2])
                max_l3 = max(max_l3, e[0][2])
            collation_elements = map(lambda x: x[0], collation_elements)
            ducet[cps] = collation_elements

    # If you're seeing an error because of this return, it's because we could
    # not determine a range of variable weights.  Hopefully, this will never
    # happen.
    if min_non_var <= max_var:
        return None

    min_l1 = min(min_var, min_non_var)
    max_l1 = max(max_var, max_non_var)

    return (ducet, min_var, max_var, min_l1, max_l1, min_l2, max_l2, min_l3, max_l3)

def ccc(cccs_dict, cp):
    if cp in cccs_dict:
        return cccs_dict[cp]
    return 0

# http://www.unicode.org/reports/tr10/#Implicit_Weights
def implicit_ce(cp):
    # Tangut and Tangut Components
    if 0x17000 <= cp and cp <= 0x18AFF:
        return [
            [0xFB00, 0x0020, 0x0020],
            [(cp - 0x17000) | 0x8000, 0x0000, 0x0000]
        ]

    # Nushu
    if 0x1B170 <= cp and cp <= 0x1B2FF:
        return [
            [0xFB00, 0x0020, 0x0020],
            [(cp - 0x1B170) | 0x8000, 0x0000, 0x0000]
        ]

    BBBB = [(cp & 0x7FFF) | 0x8000, 0x0000, 0x0000]

    # Core Han Unified Ideographs
    CJK_Compatibility_Ideographs = [
        0xFA0E, 0xFA0F, 0xFA11, 0xFA13, 0xFA14, 0xFA1F, 0xFA21, 0xFA23,
        0xFA24, 0xFA27, 0xFA28, 0xFA29
    ]
    if 0x4E00 <= cp and cp <= 0x9FEA or cp in CJK_Compatibility_Ideographs:
        return [
            [0xFB40 + (cp >> 15), 0x0020, 0x0020],
            BBBB
        ]

    CJK_Unified_Ideographs_Extension_D = [
        0x2B740, 0x2B741, 0x2B742, 0x2B743, 0x2B744, 0x2B745, 0x2B746, 0x2B747,
        0x2B748, 0x2B749, 0x2B74A, 0x2B74B, 0x2B74C, 0x2B74D, 0x2B74E, 0x2B74F,
        0x2B750, 0x2B751, 0x2B752, 0x2B753, 0x2B754, 0x2B755, 0x2B756, 0x2B757,
        0x2B758, 0x2B759, 0x2B75A, 0x2B75B, 0x2B75C, 0x2B75D, 0x2B75E, 0x2B75F,
        0x2B760, 0x2B761, 0x2B762, 0x2B763, 0x2B764, 0x2B765, 0x2B766, 0x2B767,
        0x2B768, 0x2B769, 0x2B76A, 0x2B76B, 0x2B76C, 0x2B76D, 0x2B76E, 0x2B76F,
        0x2B770, 0x2B771, 0x2B772, 0x2B773, 0x2B774, 0x2B775, 0x2B776, 0x2B777,
        0x2B778, 0x2B779, 0x2B77A, 0x2B77B, 0x2B77C, 0x2B77D, 0x2B77E, 0x2B77F,
        0x2B780, 0x2B781, 0x2B782, 0x2B783, 0x2B784, 0x2B785, 0x2B786, 0x2B787,
        0x2B788, 0x2B789, 0x2B78A, 0x2B78B, 0x2B78C, 0x2B78D, 0x2B78E, 0x2B78F,
        0x2B790, 0x2B791, 0x2B792, 0x2B793, 0x2B794, 0x2B795, 0x2B796, 0x2B797,
        0x2B798, 0x2B799, 0x2B79A, 0x2B79B, 0x2B79C, 0x2B79D, 0x2B79E, 0x2B79F,
        0x2B7A0, 0x2B7A1, 0x2B7A2, 0x2B7A3, 0x2B7A4, 0x2B7A5, 0x2B7A6, 0x2B7A7,
        0x2B7A8, 0x2B7A9, 0x2B7AA, 0x2B7AB, 0x2B7AC, 0x2B7AD, 0x2B7AE, 0x2B7AF,
        0x2B7B0, 0x2B7B1, 0x2B7B2, 0x2B7B3, 0x2B7B4, 0x2B7B5, 0x2B7B6, 0x2B7B7,
        0x2B7B8, 0x2B7B9, 0x2B7BA, 0x2B7BB, 0x2B7BC, 0x2B7BD, 0x2B7BE, 0x2B7BF,
        0x2B7C0, 0x2B7C1, 0x2B7C2, 0x2B7C3, 0x2B7C4, 0x2B7C5, 0x2B7C6, 0x2B7C7,
        0x2B7C8, 0x2B7C9, 0x2B7CA, 0x2B7CB, 0x2B7CC, 0x2B7CD, 0x2B7CE, 0x2B7CF,
        0x2B7D0, 0x2B7D1, 0x2B7D2, 0x2B7D3, 0x2B7D4, 0x2B7D5, 0x2B7D6, 0x2B7D7,
        0x2B7D8, 0x2B7D9, 0x2B7DA, 0x2B7DB, 0x2B7DC, 0x2B7DD, 0x2B7DE, 0x2B7DF,
        0x2B7E0, 0x2B7E1, 0x2B7E2, 0x2B7E3, 0x2B7E4, 0x2B7E5, 0x2B7E6, 0x2B7E7,
        0x2B7E8, 0x2B7E9, 0x2B7EA, 0x2B7EB, 0x2B7EC, 0x2B7ED, 0x2B7EE, 0x2B7EF,
        0x2B7F0, 0x2B7F1, 0x2B7F2, 0x2B7F3, 0x2B7F4, 0x2B7F5, 0x2B7F6, 0x2B7F7,
        0x2B7F8, 0x2B7F9, 0x2B7FA, 0x2B7FB, 0x2B7FC, 0x2B7FD, 0x2B7FE, 0x2B7FF,
        0x2B800, 0x2B801, 0x2B802, 0x2B803, 0x2B804, 0x2B805, 0x2B806, 0x2B807,
        0x2B808, 0x2B809, 0x2B80A, 0x2B80B, 0x2B80C, 0x2B80D, 0x2B80E, 0x2B80F,
        0x2B810, 0x2B811, 0x2B812, 0x2B813, 0x2B814, 0x2B815, 0x2B816, 0x2B817,
        0x2B818, 0x2B819, 0x2B81A, 0x2B81B, 0x2B81C, 0x2B81D
    ]
    # All other Han Unified Ideographs
    if 0x3400 <= cp and cp <= 0x4DB5 or \
      0x20000 <= cp and cp <= 0x2A6D6 or \
      0x2A700 <= cp and cp <= 0x2B734 or \
      cp in CJK_Unified_Ideographs_Extension_D or \
      0x2B820 <= cp and cp <= 0x2CEA1 or \
      0x2CEB0 <= cp and cp <= 0x2EBE0:
       return [
            [0xFB80 + (cp >> 15), 0x0020, 0x0020],
            BBBB
        ]

    # Everything else (except Hangul; sigh).
    return [
        [0xFBC0 + (cp >> 15), 0x0020, 0x0020],
        BBBB
    ]

def ce(ducet, cps):
    if cps in ducet:
        return ducet[cps]
    if len(cps) != 1:
        return None
    return implicit_ce(cps[0])

# Add the ten contractions needed for well-formedness.
# See http://www.unicode.org/reports/tr10/#Well_Formed_DUCET
def add_10_contractions(ducet):
    # 0FB2 0F71 ; CE(0FB2) CE(0F71)
    ducet[(0xFB2, 0xF71)] = ce(ducet, (0x0FB2,)) + ce(ducet, (0x0F71,))
    # 0FB3 0F71 ; CE(0FB3) CE(0F71)
    ducet[(0xFB3, 0xF71)] = ce(ducet, (0x0FB3,)) + ce(ducet, (0x0F71,))

    # 0FB2 0F71 0F72 ; CE(0FB2) CE(0F71 0F72)
    ducet[(0xFB2, 0xF71, 0xF72)] = ce(ducet, (0x0FB2,)) + ce(ducet, (0x0F71, 0xF72))
    # 0FB2 0F73      ; CE(0FB2) CE(0F71 0F72)
    ducet[(0xFB2, 0xF73)] = ce(ducet, (0x0FB2,)) + ce(ducet, (0x0F71, 0xF72))
    # 0FB2 0F71 0F74 ; CE(0FB2) CE(0F71 0F74)
    ducet[(0xFB2, 0xF71, 0xF74)] = ce(ducet, (0x0FB2,)) + ce(ducet, (0x0F71, 0xF74))
    # 0xFB2 0F75     ; CE(0FB2) CE(0F71 0F74)
    ducet[(0xFB2, 0xF75)] = ce(ducet, (0x0FB2,)) + ce(ducet, (0x0F71, 0xF74))

    # 0FB3 0F71 0F72 ; CE(0FB3) CE(0F71 0F72)
    ducet[(0xFB3, 0xF71, 0xF72)] = ce(ducet, (0x0FB3,)) + ce(ducet, (0x0F71, 0xF72))
    # 0FB3 0F73      ; CE(0FB3) CE(0F71 0F72)
    ducet[(0xFB3, 0xF73)] = ce(ducet, (0x0FB3,)) + ce(ducet, (0x0F71, 0xF72))
    # 0FB3 0F71 0F74 ; CE(0FB3) CE(0F71 0F74)
    ducet[(0xFB3, 0xF71, 0xF74)] = ce(ducet, (0x0FB3,)) + ce(ducet, (0x0F71, 0xF74))
    # 0FB3 0F75      ; CE(0FB3) CE(0F71 0F74)
    ducet[(0xFB3, 0xF75)] = ce(ducet, (0x0FB3,)) + ce(ducet, (0x0F71, 0xF74))

    return ducet

def collation_elements_for_decomposition(cccs_dict, ducet, cps, handled_ducet_keys):
    longest_prefix = (cps[0],)
    for i in reversed(range(1, len(cps))):
        t = tuple(cps[0:i + 1])
        if t in ducet:
            longest_prefix = t
            break

    i = len(longest_prefix)
    while i < len(cps):
        ccc_ = ccc(cccs_dict, cps[i])
        blocked = False
        for j in range(1, i):
            if ccc(cccs_dict, cps[j]) <= ccc_:
                blocked = True
                break
        if not blocked:
            new_longest = longest_prefix + (cps[i],)
            if new_longest in ducet:
                longest_prefix = new_longest
                cps = cps[0:i] + cps[i + 1:]
            else:
                i += 1
        else:
            i += 1

    cps = cps[len(longest_prefix):]

    handled_ducet_keys.add(longest_prefix)

    return (ce(ducet, longest_prefix), cps)

# http://www.unicode.org/reports/tr10/#Avoiding_Normalization
def ucet_from_ducet_and_decompositions(cccs_dict, ducet, decomposition_mapping):
    ucet = {}

    handled_ducet_keys = set()

    for k,v in decomposition_mapping.items():
        v0 = v
        collation_elements = []
        while len(v):
            (ces, v) = collation_elements_for_decomposition(
                cccs_dict, ducet, v, handled_ducet_keys
            )
            if ces == None:
                print hex(k),map(lambda x : hex(x), v0)
            collation_elements += ces
        ucet[(k,)] = collation_elements

    for k,v in ducet.items():
        if k in handled_ducet_keys:
            continue
        ucet[k] = v

    return ucet

import ctypes
try:
    icu_caniter = ctypes.cdll.LoadLibrary('libicu_caniter.dylib')
except OSError:
    print 'Could not load libicu_caniter.dylib.  Did you remember to build it, ' + \
          'and copy it, libicuuc.60.dylib, and libicudata.60.dylib into the ' + \
          'working directory?'
    exit(1)
icu_caniter.canonical_closure.restype = ctypes.POINTER(ctypes.c_int * 1024)

def canonical_closure(tuple_):
    array_param = (ctypes.c_int * len(tuple_))()
    for i in range(len(tuple_)):
        array_param[i] = tuple_[i]
    call_result = icu_caniter.canonical_closure(array_param, ctypes.c_int(len(array_param)))
    retval = []
    i = 0
    current = []
    for i in range(len(call_result.contents)):
        x = call_result.contents[i]
        if x == 0:
            if current == []:
                break
            if current != list(tuple_):
                retval.append(current)
            current = []
        else:
            current.append(x)
    return map(lambda x: tuple(x), retval)

#print canonical_closure((493,)) # 493 == ǭ

# http://www.unicode.org/notes/tn5/#Enumerating_Equivalent_Strings
def add_canonical_closure(fcc_ucet):
    new_ucet = {}

    for k,v in fcc_ucet.items():
        new_ucet[k] = v
        for k2 in canonical_closure(k):
            new_ucet[k2] = v

    return new_ucet

cccs_dict = cccs('DerivedCombiningClass.txt')
decomposition_mapping = \
    get_decompositions('UnicodeData.txt', cccs_dict, expand_decomp_canonical)
decomposition_mapping = filter(lambda x: x[1][1], decomposition_mapping)
decomposition_mapping = map(lambda x: (x[0], x[1][0]), decomposition_mapping)
decomposition_mapping = dict(decomposition_mapping)

# TODO: Consider using allkeys_CLDR.txt.
(ducet, min_var, max_var, min_l1, max_l1, min_l2, max_l2, min_l3, max_l3) = \
  get_ducet('allkeys.txt')

hpp_file = open('collation_weights.hpp', 'w')
hpp_file.write(weights_header_form.format(
    hex(min_l1), hex(max_l1), hex(min_l2), hex(max_l2),
    hex(min_l3), hex(max_l3), hex(min_var), hex(max_var)
))

# TODO: Remove if/when allkeys_CLDR.txt is used.
ducet = add_10_contractions(ducet)
fcc_ucet = ucet_from_ducet_and_decompositions(cccs_dict, ducet, decomposition_mapping)
fcc_ucet = add_canonical_closure(fcc_ucet)

def cps_to_key(cps):
    return '{ {{ ' + ', '.join(map(lambda x: hex(x), cps)) + ' }}}}, {} }}'.format(len(cps))

def ce_to_cpp(ce):
    return 'collation_element(uint16_t({}), uint8_t({}), uint8_t({}))'.format(hex(ce[0]), hex(ce[1]), hex(ce[2]))

def ces_to_vec(ces):
    return '{ {{ ' + ', '.join(map(ce_to_cpp, ces)) + ' }}}}, {} }}'.format(len(ces))

item_strings = map(
    lambda x : '{}, {}'.format(cps_to_key(x[0]), ces_to_vec(x[1])),
    fcc_ucet.items()
)
decompositions_map = '    { ' + ' },\n    { '.join(item_strings) + ' },\n'
cpp_file = open('collation_data.cpp', 'w')
cpp_file.write(file_form.format(decompositions_map))
