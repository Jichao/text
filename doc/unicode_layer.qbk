[section The Unicode Layer]

"Unicode is hard."

['[*-- Everyone]]

Unicode is hard to implement; the algorithms are crazy.  Even as just a user
of Unicode, it can be difficult to understand how one is supposed to use
Unicode correctly.  The text layer types do much of what is described in this
section, but nicely out of view.  Unless you need to use many different
normalization and/or encoding forms, feel free to skip those portions of this
section.

A primary design goal of the Unicode layer of _Text_ is usability.  To that
end, the data model is a simple as possible.

[heading A Quick Unicode Primer]

There are multiple encoding types defined in Unicode: UTF-8, UTF-16, and
UTF-32.  A /_cu_/ is the lowest-level datum-type in your Unicode
data. Examples are a `char` in UTF-8 and a `uint32_t` in UTF-32.  A /_cp_/ is
a 32-bit unsigned value that represents a single Unicode value. Examples are
U+0041 "A" "LATIN CAPITAL LETTER A" and U+0308 " ̈" "COMBINING DIAERESIS".

There are four different Unicode normalization forms.  Normalization is
necessary because Unicode requires that certain combinations of code points be
considered identical.  For instance, the two _cps_ U+0041 U+0308 appear like
this: "Ä", and the _cp_ U+00C4 appears like this: "Ä".  Since these two
sequences are not visually distinct, all the algorithms must treat them as the
same thing.  Therefore, the operation `"\U00000041\U00000308" == "\U000000C4"`
must return `true` for the purposes of Unicode.  Normalizations exist to put
strings of _cps_ into canonical forms.

An /extended grapheme cluster/, or just /_gr_/, is a sequence of _cps_ that
appears to the end-user to be a single character.  For example, the _cps_ from
before (U+0041 U+0308) form a _gr_, since they appear when rendered to be the
single character "Ä".


[heading Unicode Versions]

There are multiple versions of Unicode, and _Text_ only supports one at a
time.  There are large volumes of data required to implement the Unicode
algorithms, and adding data for N versions of Unicode would make an already
large library larger by a factor of N.

Most Unicode data used in _Text_ come straight from the published Unicode data
files, but the collation data are taken from _cldr_, with language-specific
tailoring data taken from _ldml_ (a part of the _cldr_ project).

To find out what versions of Unicode and _cldr_ were used to generate _Text_'s
data, call [funcref boost::text::unicode_version `unicode_version`] or
[funcref boost::text::cldr_version `cldr_version`], respectively.


[heading Unicode Layer Parameter Conventions]

Most of the Unicode layer algorithms are written as typical C++ standard
algorithms; they take iterators as input and produce output via an
out-iterator.  Since ranges are the future, there are range overloads of the
algorithms that take a pair of iterators.  The Unicode algorithms all operate
on _cps_, so they take _CPIter_ iterator parameters.  The range overloads take
_CPRng_ parameters.  For convenience, overloads are provided for many of the
Unicode layer algorithms that take _GrRng_ and _GrIter_ parameters.  This
provides convenient compatability with the text layer types, like _t_ and _r_.


[section Encoding and Normalization]

[heading Encoding]

_Text_ provides conversions between UTF-8 and UTF-16, and between UTF-8 and
UTF-32, via four converting iterators:

* _from_32_iter_
* _to_32_iter_
* _from_16_iter_
* _to_16_iter_

By default, these produce the Unicode replacement character `0xFFFD` when
encoutering an invalid encoding.  This replacement character error handling
strategy is used internally within _Text_ when performing conversions.  The
exact error handling behavior can be controlled via the `ErrorHandler`
template parameter.

The Unicode standard is flexible with respect to where, in an incoming stream,
encoding errors are reported.  However, the standard provides recommendations
for where within a stream, and how frequently within that stream, errors
should be reported.  _Text_'s converting iterators follow the Unicode
recommendations.  See Unicode, "Best Practices for Using U+FFFD" and Table
3-8.

The converting iterators are pretty straightforward, but there is an important
caveat.  Because each of these converting iterators does a substantial amount
of work in increment and decrement operations, including in some cases caching
the result of reading several characters of a multi-character encoding,
post-increment and post-decrement can be quite a bit more expensive than
pre-increment and pre-decrement.

To use a converting iterator, you must provide it with an underlying iterator
that models the _CPIter_ concept.  Note that these converting iterators each
model _CPIter_ too.  An example of use:

[to_utf32_verbose]

That's a lot of typing, so there's also a much terser range-based form using _u32_rng_:

[to_utf32_terse]

There are two output iterator adapters, [funcref
boost::text::utf8::from_utf32_inserter `from_utf32_inserter`] and [funcref
boost::text::utf8::from_utf32_back_inserter `from_utf32_back_inserter`], that
do UTF-8 -> UTF-32 conversion and `push_back()` or `insert()` in one step.

Finally, there is [funcref boost::text::to_string `to_string`], which takes
two UTF-32 iterators and returns a _s_ containing the given sequence,
UTF-8-encoded.


[heading Normalization]

_Text_ provides algorithms for all four Unicode normalization forms: NFD,
NFKD, NFC, and NFKC.  In addition, it provides an unofficial fifth
normalization form called FCC that is described in _tn5_.  FCC is just as
compact as the most compact official form, NFC, except in a few degenerate
cases.  FCC is particularly useful when doing collation.

The algorithms are of the form `normalize_to_X()`, and there are range and
iterator-based interfaces.  The iterator interfaces require iterators that
model _CPIter_, and ranges that model _CPRange_.  There are also algorithms
that can check if a _cp_ sequence is in a certain normalization form.

[normalize_1]

[warning Beware!  The normalization checks attempt to use a quick-check lookup
for each code point, to avoid having to do a full normalization.  This quick
check may be inconclusive for a particular code point an normalization form.
In this case, a full normalization is performed, and the result is compared to
the input sequence.]

There are _s_-specific in-place normalization functions as well, in
_norm_str_header_.

[endsect]


[section Text Segmentation]

Unicode provides algorithms for breaking _cp_ sequences into _grs_, words,
sentences, and lines.  The Unicode Bidirectional Algorithm requires paragraph
breaking too, so paragraph breaking is included as well, even though it is not
an official Unicode text segmentation algorithm.

[heading Conventions]

All the kinds of text breaking have a common pattern.  Each kind of break `X`
provides at least these functions:

    template <typename CPIter, typename Sentinel>
    CPIter prev_X_break(CPIter first, CPIter it, Sentinel last) noexcept;

`prev_X_break()` returns `it` if `it` is already at a break, or the break
before `it` otherwise.  There is one exception to this _emdash_ even though
there is always an implicit break at the end of an sequence of _cps_, if `it`
== `last`, the previous break is still returned, if any.

This behavior allows us to do two convenient things with `prev_X_break()`.
First, we can use `prev_X_break(first, it, last) == it` as a predicate that
`it` is at a break.  Second, we can use `prev_X_break()` followed by
`next_X_break()` to find the nearest breaks that include `it`.

Note that `prev_X_break()` requires `last` because in the genereal case, the
algorithm needs to know context after `it` to determine where the breaks are
at or before `it`.

    template <typename CPIter, typename Sentinel>
    CPIter next_X_break(CPIter first, Sentinel last) noexcept;

`next_X_break()` returns the next break after `first`.  It has a precondition
that `first` is already at a break; the results are otherwise undefined.

    template<typename CPIter, typename Sentinel>
    cp_range<CPIter> X(CPIter first, CPIter it, Sentinel last) noexcept;

`X()` returns smallest range of _cps_ that comprise an `X` (word, line, etc.)
in which `it` is found.

    template<typename CPIter, typename Sentinel>
    auto Xs(CPIter first, Sentinel last) noexcept;

`Xs()` returns a lazy range of subranges of `[first, last)`.  Each subrange is an `X`.

    template<typename CPIter>
    auto reversed_Xs(CPIter first, CPIter last) noexcept;

`reversed_Xs()` returns the same thing as `Xs()`, with the subranges in
reverse order.

And of course there are _CPRange_ overloads as well:

    template<typename CPRange, typename CPIter>
    auto prev_X_break(CPRange & range, CPIter it) noexcept;
    template<typename CPRange, typename CPIter>
    auto next_X_break(CPRange & range, CPIter it) noexcept;
    template<typename CPRange, typename CPIter>
    auto X(CPRange & range, CPIter it) noexcept;
    template<typename CPRange>
    auto Xs(CPRange & range) noexcept;
    template<typename CPRange>
    auto reversed_Xs(CPRange & range) noexcept;

For all kinds of breaks besides grapheme breaks, there are range overloads
that accept _GrRng_ ranges _GrIter_ iterators instead.  These provide
convenient support for using the Unicode layer algorithms with the text layer
types link _t_ and _r_.

    template<typename GraphemeRange, typename GraphemeIter>
    auto prev_X_break(GraphemeRange const & range, GraphemeIter it) noexcept;
    template<typename GraphemeRange, typename GraphemeIter>
    auto next_X_break(GraphemeRange const & range, GraphemeIter it) noexcept;
    template<typename GraphemeRange, typename GraphemeIter>
    auto X(GraphemeRange const & range, GraphemeIter it) noexcept;
    template<typename GraphemeRange>
    auto Xs(GraphemeRange const & range) noexcept;
    template<typename GraphemeRange>
    auto reversed_Xs(GraphemeRange const & range) noexcept;

[heading Tailoring]

Unicode allows for /tailoring/ of the segmentation algorithms, to produce
customized results that are necessary or useful for a particular application,
or to produce correct results in cases that the Unicode algorithms do not
handle.  Some of the break algorithms below are tailorable.  Each section
below indicates whether a certain kind of break is tailorable, and if so, how.

[heading Graphemes]

[grapheme_breaks]

_Text_ does not support tailoring of _gr_ breaking, because _grs_ are the
fundamental unit of work for the text layer of the library.  Everyone must
have the same notion of what a _gr_ is for that to work.

[heading Stream-Safe Format]

Consider this sequence of _cps_: the letter `'a'` followed by 10,000
/umlauts/.  There is no technical reason why you cannot create this sequence
of _cps_, though it is not useful to do so.  This creates a problem for
algorithms like normalization or _gr_ breaking, because they may be required
to look ahead a very long way in order to determine how to handle the current
_gr_.  To address this, Unicode allows a conforming implementation to assume
that an sequence of _cps_ contains _grs_ of at most 32 _cps_.  This is known
as the _str_safe_ assumption; _Text_ makes this assumption.

[heading Stream-Safe Format and Security]

If you Give _Text_ algorithms a _cp_ sequence with _grs_ longer than 32 _cps_,
you will get undefined behavior.  This poses a security problem.  To address
this, there is a simple algorithm described at _str_safe_ that can put any
_cp_ sequence into the _str_safe_.  You may want to implement this and use it
with text from an unknown source.

[heading Words]

Word break occur where you'd expect _emdash_ at the beginnings and ends of
words _emdash_ but that implies that they occur where you might not expect
_emdash_ at the beginnings and ends of the _cp_ sequences *between* words.
Here is an example of word breaks taken from _text_seg_.  The string `"The
quick (“brown”) fox can’t jump 32.3 feet, right?"` is broken up into words
like this:

[table Word Break Example
    [
    [`"The"`]
    [`" "`]
    [`"quick"`]
    [`" "`]
    [`"("`]
    [`"“"`]
    [`"brown"`]
    [`"”"`]
    [`")"`]
    [`" "`]
    [`"fox"`]
    [`" "`]
    [`"can’t"`]
    [`" "`]
    [`"jump"`]
    [`" "`]
    [`"32.3"`]
    [`" "`]
    [`"feet"`]
    [`","`]
    [`" "`]
    [`"right"`]
    [`"?"`]
    ]
]

Note that many of those "words" are not what most people would consider to be
words.  You may need to do some additional processing to find only the "real"
words, if that matters in your use case.

The word breaking API can be used just as the _gr_ break API, except that it
also has _GrRng_ overloads.  Here are some example calls using only the
_GrRng_ overloads, with a _t_ as the _GrRng_:

[word_breaks_1]

[heading Limitations of Word Breaking]

This algorithm does not work for all languages.  From _text_seg_:

[:For Thai, Lao, Khmer, Myanmar, and other scripts that do not typically use
spaces between words, a good implementation should not depend on the default
word boundary specification. It should use a more sophisticated mechanism, as
is also required for line breaking. Ideographic scripts such as Japanese and
Chinese are even more complex. Where Hangul text is written without spaces,
the same applies. However, in the absence of a more sophisticated mechanism,
the rules specified in this annex supply a well-defined default.]

French and Italian words are not broken after an apostrophe; the default
algorithm finds `"l’objectif"` to be a single word.

Breaking on dashes in the default; the default algorithm finds
`"out-of-the-box"` to be seven words.

There are other rarer failure cases in that document you might want to look at
too.

[heading Word Break Tailoring]

Forunately, unlike _gr_ breaking, word breaking is tailorable in two ways.

Each break algorithm is defined in terms of _cp_ properties; each _cp_ is a
letter, digit, punctuation, etc.  All the work break functions accept an
optional word-property lookup function to replace the default one.

For example, here I've made a custom word property lookup function that treats
a regular dash `'-'` as a `MidLetter`, meaning a _cp_ that is part of a word
as long as it can reach at least one letter before reaching a line break on
either side:

[word_breaks_2]

From _text_seg_, here are some other _cps_ you might want to treat as
`MidLetter`, depending on your language and use case:

[table `MidLetter` Candidates
    [[Code Point]]
    [[U+002D ( - ) HYPHEN-MINUS]]
    [[U+055A ( ՚ ) ARMENIAN APOSTROPHE]]
    [[U+058A ( ֊ ) ARMENIAN HYPHEN]]
    [[U+0F0B ( ་ ) TIBETAN MARK INTERSYLLABIC TSHEG]]
    [[U+1806 ( ᠆ ) MONGOLIAN TODO SOFT HYPHEN]]
    [[U+2010 ( ‐ ) HYPHEN]]
    [[U+2011 ( ‑ ) NON-BREAKING HYPHEN]]
    [[U+201B ( ‛ ) SINGLE HIGH-REVERSED-9 QUOTATION MARK]]
    [[U+30A0 ( ゠ ) KATAKANA-HIRAGANA DOUBLE HYPHEN]]
    [[U+30FB ( ・ ) KATAKANA MIDDLE DOT]]
    [[U+FE63 ( ﹣ ) SMALL HYPHEN-MINUS]]
    [[U+FF0D ( － ) FULLWIDTH HYPHEN-MINUS]]
]

Another example from _text_seg_ is to treat spaces as `MidNum` to support
languages that use spaces as thousands separators, as in `"€1 234,56"`.
`MidNum` is like `MidLetter`, but for the interior _cps_ of numbers instead of
words containing letters.  Here are the space characters you might want to do
that with:

[table `MidNum` Candidates
    [[Code Point]]
    [[U+0020 SPACE]]
    [[U+00A0 NO-BREAK SPACE ]]
    [[U+2007 FIGURE SPACE]]
    [[U+2008 PUNCTUATION SPACE]]
    [[U+2009 THIN SPACE]]
    [[U+202F NARROW NO-BREAK SPACE]]
]

Tailoring the properties for each code point works for some cases, but using
tailorings of the meanings of `MidLetter` and `MidNum` can only add to the
sizes of words; it cannot decrease their sizes.  The word break functions take
a second optional parameter that allows you to pick arbitrary word breaks
based on limited context.

The _Text_ implementation of the word break algorithm uses the current _cp_,
plus two _cps_ before and two _cps_ after, to determine whether a word break
exists at the current _cp_.  Therefore, the signature of the custom word break
function is this:

    bool custom_break(uint32_t prev_prev,
                      uint32_t prev,
                      uint32_t curr,
                      uint32_t next,
                      uint32_t next_next);

Returning `true` indicates that `[prev, curr]` straddles a word break; `prev`
is the last _cp_ of one word, and `curr` is the first _cp_ of the next:

[word_breaks_3]

[heading Sentences]

The sentence breaking API is the same as the word breaking API, without the
extra tailoring parameters.

[heading Paragraphs]

The paragraph breaking API is the same as the sentence and word breaking APIs,
without the extra tailoring parameters.

Unicode does not list paragraph breaks as a specific kind of text
segmentation, but it can be useful in some cases.  In particular, paragraph
detection is part of the Unicode bidirectional algorithm.  One way of
tailoring the behavior of the bidirectional algorithm is to process some
paragraphs separately from others; having an API for detecting paragraph
breaks makes that simpler.

[heading Lines]

The Unicode line breaking algorithm differs from the other break algorithms in
that there are multiple kinds of line breaks.  Some line breaks are required,
as after a newline (e.g. `"\n"` or `"\r\n"`).  These are known as /hard/ line
breaks.

The line breaking algorithm produces many more line breaks, but all non-hard
line breaks are places where it is possible to break the line _emdash_ though
it is not necessary to do so.  These are known as /allowed/ line breaks.
Higher-level program logic must determine which of these allowed breaks is to
be used, for example to fit in available horizontal space.

[note _Text_ only generates hard line breaks where they are indicated in the
Unicode line breaking rules *and* there could be an allowed line break.  Line
breaks always occur at the beginning and end of any sequence, but _Text_ never
reports those as hard breaks.]


[line_breaks]

TODO

[endsect]


[section Case Mapping]

TODO

[endsect]


[section Collation]

TODO

[endsect]


[section Searching]

TODO

[endsect]


[section Bidirectional Text]

TODO

[endsect]

[endsect]
