[section The Unicode Layer]

"Unicode is hard."

['[*-- Everyone]]

Unicode is hard to implement; the algorithms are crazy.  Even as just a user
of Unicode, it can be difficult to understand how one is supposed to use
Unicode correctly.  The text layer types do much of what is described in this
section, but nicely out of view.  Unless you need to use many different
normalization and/or encoding forms, feel free to skip those portions of this
section.

A primary design goal of the Unicode layer of _Text_ is usability.  To that
end, the data model is a simple as possible.

[heading A Quick Unicode Primer]

There are multiple encoding types defined in Unicode: UTF-8, UTF-16, and
UTF-32.  A /_cu_/ is the lowest-level datum-type in your Unicode
data. Examples are a `char` in UTF-8 and a `uint32_t` in UTF-32.  A /_cp_/ is
a 32-bit unsigned value that represents a single Unicode value. Examples are
U+0041 "A" "LATIN CAPITAL LETTER A" and U+0308 " ̈" "COMBINING DIAERESIS".

There are four different Unicode normalization forms.  Normalization is
necessary because Unicode requires that certain combinations of code points be
considered identical.  For instance, the two _cps_ U+0041 U+0308 appear like
this: "Ä", and the _cp_ U+00C4 appears like this: "Ä".  Since these two
sequences are not visually distinct, all the algorithms must treat them as the
same thing.  Therefore, the operation `"\U00000041\U00000308" == "\U000000C4"`
must return `true` for the purposes of Unicode.  Normalizations exist to put
strings of _cps_ into canonical forms.

An /extended grapheme cluster/, or just /_gr_/, is a sequence of _cps_ that
appears to the end-user to be a single character.  For example, the _cps_ from
before (U+0041 U+0308) form a _gr_, since they appear when rendered to be the
single character "Ä".


[heading Unicode Versions]

There are multiple versions of Unicode, and _Text_ only supports one at a
time.  There are large volumes of data required to implement the Unicode
algorithms, and adding data for N versions of Unicode would make an already
large library larger by a factor of N.

Most Unicode data used in _Text_ come straight from the published Unicode data
files, but the collation data are taken from _cldr_, with language-specific
tailoring data taken from _ldml_ (a part of the _cldr_ project).

To find out what versions of Unicode and _cldr_ were used to generate _Text_'s
data, call [funcref boost::text::unicode_version `unicode_version`] or
[funcref boost::text::cldr_version `cldr_version`], respectively.


[heading Unicode Layer Parameter Conventions]

Most of the Unicode layer algorithms are written as typical C++ standard
algorithms; they take iterators as input and produce output via an
out-iterator.  Since ranges are the future, there are range overloads of the
algorithms that take a pair of iterators.  The Unicode algorithms all operate
on _cps_, so they take _CPIter_ iterator parameters.  The range overloads take
_CPRng_ parameters.  For convenience, overloads are provided for many of the
Unicode layer algorithms that take _GrRng_ and _GrIter_ parameters.  This
provides convenient compatability with the text layer types, like _t_ and _r_.


[section Encoding and Normalization]

[heading Encoding]

_Text_ provides conversions between UTF-8 and UTF-16, and between UTF-8 and
UTF-32, via four converting iterators:

* _from_32_iter_
* _to_32_iter_
* _from_16_iter_
* _to_16_iter_

By default, these produce the Unicode replacement character `0xFFFD` when
encoutering an invalid encoding.  This replacement character error handling
strategy is used internally within _Text_ when performing conversions.  The
exact error handling behavior can be controlled via the `ErrorHandler`
template parameter.

The Unicode standard is flexible with respect to where, in an incoming stream,
encoding errors are reported.  However, the standard provides recommendations
for where within a stream, and how frequently within that stream, errors
should be reported.  _Text_'s converting iterators follow the Unicode
recommendations.  See Unicode, "Best Practices for Using U+FFFD" and Table
3-8.

The converting iterators are pretty straightforward, but there is an important
caveat.  Because each of these converting iterators does a substantial amount
of work in increment and decrement operations, including in some cases caching
the result of reading several characters of a multi-character encoding,
post-increment and post-decrement can be quite a bit more expensive than
pre-increment and pre-decrement.

To use a converting iterator, you must provide it with an underlying iterator
that models the _CPIter_ concept.  Note that these converting iterators each
model _CPIter_ too.  An example of use:

[to_utf32_verbose]

That's a lot of typing, so there's also a much terser range-based form using _u32_rng_:

[to_utf32_terse]

There are two output iterator adapters, [funcref
boost::text::utf8::from_utf32_inserter `from_utf32_inserter`] and [funcref
boost::text::utf8::from_utf32_back_inserter `from_utf32_back_inserter`], that
do UTF-8 -> UTF-32 conversion and `push_back()` or `insert()` in one step.

Finally, there is [funcref boost::text::to_string `to_string`], which takes
two UTF-32 iterators and returns a _s_ containing the given sequence,
UTF-8-encoded.


[heading Normalization]

_Text_ provides algorithms for all four Unicode normalization forms: NFD,
NFKD, NFC, and NFKC.  In addition, it provides an unofficial fifth
normalization form called FCC that is described in _tn5_.  FCC is just as
compact as the most compact official form, NFC, except in a few degenerate
cases.  FCC is particularly useful when doing collation.

The algorithms are of the form `normalize_to_X()`, and there are range and
iterator-based interfaces.  The iterator interfaces require iterators that
model _CPIter_, and ranges that model _CPRange_.  There are also algorithms
that can check if a _cp_ sequence is in a certain normalization form.

[normalize_1]

[warning Beware!  The normalization checks attempt to use a quick-check lookup
for each code point, to avoid having to do a full normalization.  This quick
check may be inconclusive for a particular code point an normalization form.
In this case, a full normalization is performed, and the result is compared to
the input sequence.]

There are _s_-specific in-place normalization functions as well, in
_norm_str_header_.

[endsect]


[section Text Segmentation]

Unicode provides algorithms for breaking _cp_ sequences into _grs_, words,
sentences, and lines.  The Unicode Bidirectional Algorithm requires paragraph
breaking too, so paragraph breaking is included as well, even though it is not
an official Unicode text segmentation algorithm.

[heading Conventions]

All the kinds of text breaking have a common pattern.  Each kind of break `X`
provides at least these functions:

    template <typename CPIter, typename Sentinel>
    CPIter prev_X_break(CPIter first, CPIter it, Sentinel last) noexcept;

`prev_X_break()` returns `it` if `it` is already at a break, or the break
before `it` otherwise.  There is one exception to this _emdash_ even though
there is always an implicit break at the end of an sequence of _cps_, if `it`
== `last`, the previous break is still returned, if any.

This behavior allows us to do two convenient things with `prev_X_break()`.
First, we can use `prev_X_break(first, it, last) == it` as a predicate that
`it` is at a break.  Second, we can use `prev_X_break()` followed by
`next_X_break()` to find the nearest breaks that include `it`.

Note that `prev_X_break()` requires `last` because in the genereal case, the
algorithm needs to know context after `it` to determine where the breaks are
at or before `it`.

    template <typename CPIter, typename Sentinel>
    CPIter next_X_break(CPIter first, Sentinel last) noexcept;

`next_X_break()` returns the next break after `first`.  It has a precondition
that `first` is already at a break; the results are otherwise undefined.

    template<typename CPIter, typename Sentinel>
    cp_range<CPIter> X(CPIter first, CPIter it, Sentinel last) noexcept;

`X()` returns smallest range of _cps_ that comprise an `X` (word, line, etc.)
in which `it` is found.

    template<typename CPIter, typename Sentinel>
    auto Xs(CPIter first, Sentinel last) noexcept;

`Xs()` returns a lazy range of subranges of `[first, last)`.  Each subrange is an `X`.

    template<typename CPIter>
    auto reversed_Xs(CPIter first, CPIter last) noexcept;

`reversed_Xs()` returns the same thing as `Xs()`, with the subranges in
reverse order.

And of course there are _CPRange_ overloads as well:

    template<typename CPRange, typename CPIter>
    auto prev_X_break(CPRange & range, CPIter it) noexcept;
    template<typename CPRange, typename CPIter>
    auto next_X_break(CPRange & range, CPIter it) noexcept;
    template<typename CPRange, typename CPIter>
    auto X(CPRange & range, CPIter it) noexcept;
    template<typename CPRange>
    auto Xs(CPRange & range) noexcept;
    template<typename CPRange>
    auto reversed_Xs(CPRange & range) noexcept;

For all kinds of breaks besides grapheme breaks, there are range overloads
that accept _GrRng_ ranges _GrIter_ iterators instead.  These provide
convenient support for using the Unicode layer algorithms with the text layer
types link _t_ and _r_.

    template<typename GraphemeRange, typename GraphemeIter>
    auto prev_X_break(GraphemeRange const & range, GraphemeIter it) noexcept;
    template<typename GraphemeRange, typename GraphemeIter>
    auto next_X_break(GraphemeRange const & range, GraphemeIter it) noexcept;
    template<typename GraphemeRange, typename GraphemeIter>
    auto X(GraphemeRange const & range, GraphemeIter it) noexcept;
    template<typename GraphemeRange>
    auto Xs(GraphemeRange const & range) noexcept;
    template<typename GraphemeRange>
    auto reversed_Xs(GraphemeRange const & range) noexcept;

[heading Tailoring]

Unicode allows for /tailoring/ of the segmentation algorithms, to produce
customized results that are necessary or useful for a particular application,
or to produce correct results in cases that the Unicode algorithms do not
handle.  Some of the break algorithms below are tailorable.  Each section
below indicates whether a certain kind of break is tailorable, and if so, how.

[heading Graphemes]

[grapheme_breaks]

_Text_ does not support tailoring of _gr_ breaking, because _grs_ are the
fundamental unit of work for the text layer of the library.  Everyone must
have the same notion of what a _gr_ is for that to work.

[heading Stream-Safe Format]

Consider this sequence of _cps_: the letter `'a'` followed by 10,000
/umlauts/.  There is no technical reason why you cannot create this sequence
of _cps_, though it is not useful to do so.  This creates a problem for
algorithms like normalization or _gr_ breaking, because they may be required
to look ahead a very long way in order to determine how to handle the current
_gr_.  To address this, Unicode allows a conforming implementation to assume
that an sequence of _cps_ contains _grs_ of at most 32 _cps_.  This is known
as the _str_safe_ assumption; _Text_ makes this assumption.

[heading Stream-Safe Format and Security]

If you Give _Text_ algorithms a _cp_ sequence with _grs_ longer than 32 _cps_,
you will get undefined behavior.  This poses a security problem.  To address
this, there is a simple algorithm described at _str_safe_ that can put any
_cp_ sequence into the _str_safe_.  You may want to implement this and use it
with text from an unknown source.

[heading Words]

Word break occur where you'd expect _emdash_ at the beginnings and ends of
words _emdash_ but that implies that they occur where you might not expect
_emdash_ at the beginnings and ends of the _cp_ sequences *between* words.
Here is an example of word breaks taken from _text_seg_.  The string `"The
quick (“brown”) fox can’t jump 32.3 feet, right?"` is broken up into words
like this:

[table Word Break Example
    [
    [`"The"`]
    [`" "`]
    [`"quick"`]
    [`" "`]
    [`"("`]
    [`"“"`]
    [`"brown"`]
    [`"”"`]
    [`")"`]
    [`" "`]
    [`"fox"`]
    [`" "`]
    [`"can’t"`]
    [`" "`]
    [`"jump"`]
    [`" "`]
    [`"32.3"`]
    [`" "`]
    [`"feet"`]
    [`","`]
    [`" "`]
    [`"right"`]
    [`"?"`]
    ]
]

Note that many of those "words" are not what most people would consider to be
words.  You may need to do some additional processing to find only the "real"
words, if that matters in your use case.

The word breaking API can be used just as the _gr_ break API, except that it
also has _GrRng_ overloads.  Here are some example calls using only the
_GrRng_ overloads, with a _t_ as the _GrRng_:

[word_breaks_1]

[heading Limitations of Word Breaking]

This algorithm does not work for all languages.  From _text_seg_:

[:For Thai, Lao, Khmer, Myanmar, and other scripts that do not typically use
spaces between words, a good implementation should not depend on the default
word boundary specification. It should use a more sophisticated mechanism, as
is also required for line breaking. Ideographic scripts such as Japanese and
Chinese are even more complex. Where Hangul text is written without spaces,
the same applies. However, in the absence of a more sophisticated mechanism,
the rules specified in this annex supply a well-defined default.]

French and Italian words are not broken after an apostrophe; the default
algorithm finds `"l’objectif"` to be a single word.

Breaking on dashes in the default; the default algorithm finds
`"out-of-the-box"` to be seven words.

There are other rarer failure cases in that document you might want to look at
too.

[heading Word Break Tailoring]

Forunately, unlike _gr_ breaking, word breaking is tailorable in two ways.

Each break algorithm is defined in terms of _cp_ properties; each _cp_ is a
letter, digit, punctuation, etc.  All the work break functions accept an
optional word-property lookup function to replace the default one.

For example, here I've made a custom word property lookup function that treats
a regular dash `'-'` as a `MidLetter`, meaning a _cp_ that is part of a word
as long as it can reach at least one letter before reaching a line break on
either side:

[word_breaks_2]

From _text_seg_, here are some other _cps_ you might want to treat as
`MidLetter`, depending on your language and use case:

[table `MidLetter` Candidates
    [[Code Point]]
    [[U+002D ( - ) HYPHEN-MINUS]]
    [[U+055A ( ՚ ) ARMENIAN APOSTROPHE]]
    [[U+058A ( ֊ ) ARMENIAN HYPHEN]]
    [[U+0F0B ( ་ ) TIBETAN MARK INTERSYLLABIC TSHEG]]
    [[U+1806 ( ᠆ ) MONGOLIAN TODO SOFT HYPHEN]]
    [[U+2010 ( ‐ ) HYPHEN]]
    [[U+2011 ( ‑ ) NON-BREAKING HYPHEN]]
    [[U+201B ( ‛ ) SINGLE HIGH-REVERSED-9 QUOTATION MARK]]
    [[U+30A0 ( ゠ ) KATAKANA-HIRAGANA DOUBLE HYPHEN]]
    [[U+30FB ( ・ ) KATAKANA MIDDLE DOT]]
    [[U+FE63 ( ﹣ ) SMALL HYPHEN-MINUS]]
    [[U+FF0D ( － ) FULLWIDTH HYPHEN-MINUS]]
]

Another example from _text_seg_ is to treat spaces as `MidNum` to support
languages that use spaces as thousands separators, as in `"€1 234,56"`.
`MidNum` is like `MidLetter`, but for the interior _cps_ of numbers instead of
words containing letters.  Here are the space characters you might want to do
that with:

[table `MidNum` Candidates
    [[Code Point]]
    [[U+0020 SPACE]]
    [[U+00A0 NO-BREAK SPACE ]]
    [[U+2007 FIGURE SPACE]]
    [[U+2008 PUNCTUATION SPACE]]
    [[U+2009 THIN SPACE]]
    [[U+202F NARROW NO-BREAK SPACE]]
]

Tailoring the properties for each code point works for some cases, but using
tailorings of the meanings of `MidLetter` and `MidNum` can only add to the
sizes of words; it cannot decrease their sizes.  The word break functions take
a second optional parameter that allows you to pick arbitrary word breaks
based on limited context.

The _Text_ implementation of the word break algorithm uses the current _cp_,
plus two _cps_ before and two _cps_ after, to determine whether a word break
exists at the current _cp_.  Therefore, the signature of the custom word break
function is this:

    bool custom_break(uint32_t prev_prev,
                      uint32_t prev,
                      uint32_t curr,
                      uint32_t next,
                      uint32_t next_next);

Returning `true` indicates that `[prev, curr]` straddles a word break; `prev`
is the last _cp_ of one word, and `curr` is the first _cp_ of the next:

[word_breaks_3]

[heading Sentences]

The sentence breaking API is the same as the word breaking API, without the
extra tailoring parameters.

[heading Paragraphs]

The paragraph breaking API is the same as the sentence and word breaking APIs,
without the extra tailoring parameters.

Unicode does not list paragraph breaks as a specific kind of text
segmentation, but it can be useful in some cases.  In particular, paragraph
detection is part of the Unicode bidirectional algorithm.  One way of
tailoring the behavior of the bidirectional algorithm is to process some
paragraphs separately from others; having an API for detecting paragraph
breaks makes that simpler.

[heading Lines]

The Unicode line breaking algorithm differs from the other break algorithms in
that there are multiple kinds of line breaks.  Some line breaks are required,
as after a newline (e.g. `"\n"` or `"\r\n"`).  These are known as /hard/ line
breaks.

The line breaking algorithm produces many more line breaks, but all non-hard
line breaks are places where it is possible to break the line _emdash_ though
it is not necessary to do so.  These are known as /allowed/ line breaks.
Higher-level program logic must determine which of these allowed breaks is to
be used, for example to fit in available horizontal space.

[note _Text_ only generates hard line breaks where they are indicated in the
Unicode line breaking rules *and* there could be an allowed line break.  Line
breaks always occur at the beginning and end of any sequence, but _Text_ does
not report those as hard breaks.]

The `next_*_break()` and `prev_*_break()` functions for line breaking come in
two flavors.  There are `hard_line` versions and `allowed_line` versions.  For
the `allowed` overloads, you may need to know, once you have the break
position, whether it was a hard line break.  The `allowed` overloads therefore
return a struct, _line_brk_res_.  It has an `.iter` member to indicate the
location, and a `.hard_break` member ot indicate whether that location is a
hard line break.  Overloads of `operator==()` ans `operator!=()` are defined
between _line_brk_res_ and its iterator type so that you can treat it as an
iterator in generic code if you don't care about the hard line break
information:

[line_breaks_1]

The `hard` naming is only present in these low-level functions; the rest of
the line breaking APi uses `line` for the hard break version, and
`allowed_line` for the other.  The reset of the line breaking API should be
familiar by now; it parallels the other breaking APIs, but with the hard
vs. allowed overloads.

Just as the low-level `prev` and `next` functions for allowed beaks returned
extra information, the other allowed-break functions do as well.  The rest of
the API produces _cp_ ranges, and the allowed-break versions produce
_line_brk_cp_rngs_ instead:

[line_breaks_2]

Additionally, there are overloads that make it convenient to write simple code
that selects an allowed break based on available space.  The available space,
and the amout of space taken up by each check of _cps_, is user-configurable.
There is an overload of `lines()` that you pass the amount of space and a
callable that determines the space used by some sequence of _cps_.  Each chunk
contains the _cps_ between allowed breaks.  If a chunk would exceed available
space, the allowed break before that chunk is used:

[line_breaks_3]

[endsect]


[section Case Mapping]

Case mapping is conceptually simple.  There are three kinds of case:
lower-case, upper-case, and title-case.  Title-case has an upper-case letter
at the beginning of each word.  There are six operations, though there are a
few overloads of each.  There are three case-mapping algorithms: _to_lower_,
_to_title_, and _to_upper_.  Each of these outputs its result (as _cps_) via
an out-iterator.  There are also three case predicates, _is_lower_,
_is_title_, and _is_upper_.

For each of these, there are overloads that take the input _cp_ sequence as a
pair of _CPIters_, a _CPRng_, or a _GrRng_:

[case_mapping_1]

As a complication, some languages have case-mapping rules that differ from the
general case, and so there is an optional _case_lang_ parameter to the
`to_*()` functions that you can specify to get this custom behavior:

[case_mapping_2]

Another complication is that the title-case functions need to know where word
boundaries are.  By default, they use an instance of _wbreak_call_, which in
turn just calls _next_wbreak_.  You can supply your own callable instead if
you need tailored word breaking.

There are a few case mapping behaviors that are common in various languages,
but that are not accounted for by the default Unicode case mapping rules.  For
instance, Dutch capitalizes `"IJ"` at the beginning of title-cased words.
This is available in _Text_'s implementation if you use `_case_lang_::dutch`.
_Text_ also implements the somewhat complicated rules for upper-casing modern
Greek.

Finally, there are in-place versions of the case-mapping functions available
for use with _t_ and _r_:

[case_mapping_3]

[endsect]


[section Collation]

Collation is the relative ordering of sequences of _cps_ for purposes of
sorting or searching.  The Unicode collation algorithm takes a sequence of
_cps_ and produces a sequence of numbers (a "sort key") that can be
lexicographically compared to another sequence's sort key.

Why can't we just lexicographically compare two Unicode strings?  Because
Unicode.  Consider two _cps_ `A` and `B`.  There may be some languages for
which the proper ordering of `A` and `B` is `A < B`.  There may also be other
languages for which `B < A` is the proper order.  Ther may be yet other
languages for which `A == B` is the proper order.  As a concrete example, in
Swedish `z < ö`, whereas in German `ö < z`.

So, if I want to implement a simple function like this:

    // Compare two code points for dictionary ordering.
    bool impossible_less(uint32_t lhs, uint32_t rhs)
    {
        return /* What goes here? */;
    }

I can't, because I don't know what language we're using these _cps_ in.

Sadly, collation is even more complicated than this.  Collation must also
handle the different ordering priorities of different characteristics of _cps_
within a single language or context.  For instance, I may want capitals first,
implying that `G < g`, or I may want capitals last, implying `g < G`.  Some
languages sort based on accents, and some do not; collation must know whether
`o < ô` or `o == ô`.

To handle this correctly, collation sort keys are created with support for
four levels of comparison.  The primary level (/L1/) represents differences in
the base character being represented; the secondary level (/L2/) represents
differences in accent; the tertiary level (/L3/) represents differences in
case, or variants of characters; and the quaternary level (/L4/) represents
differences in punctuation.

From the Unicde documentation:

[table Comparison Levels
    [[Level] [Description]     [Examples]]
    [[L1]    [Base characters] [role < roles < rule]]
    [[L2]    [Accents]         [role < rôle < roles]]
    [[L3]    [Case/Variants]   [role < Role < rôle]]
    [[L4]    [Punctuation]     [role < “role” < Role]]
]

When forming a sort key, all the L1 weights come first _emdash_ for all _cps_
in the sequence _emdash_ then all the L2 weights, then the L3 weights, etc.
This means that any L1 difference is treated as more important than any L2
difference, and any L2 difference trumps any L3 difference, etc.

It is possible to compare sequences considering only a contiguous subrange of
levels; this is known as the collation /strength/.  For example, L1 strength
means "Ignore accents, case, and punctuation", and L2 strength means "Ignore
case and punctuation".

There are also parameters you can provide to the collation algorithm that
create variations such as "Ignore accents, but do consider case".

[heading Tailoring]

Since there exists no unique mapping of _cps_ to collation weights that works
for every language and context, there needs to be a means available to users
of the Unicode collation algorithm of tailoring collation to a particular
language or use case.  _Text_ supports the _ldml_ format for specifying
collation tailoring; see that webstire for details.  An example of this super
simple and convenient format is:

    [normalization on]
    [reorder Grek]
    &N<ñ<<<Ñ
    &C<ch<<<Ch<<<CH
    &l<ll<<<Ll<<<LL

You should never need to write one of these tailoring scripts, but if you do,
there's a full parser of the tailoring scripting language built in to _Text_.
For most users, it should be sufficient to use one of the canned tailoring
scripts in the `boost/text/data/` directory.  These come from _cldr_, and the
files use the _cldr_ locale naming scheme.  Just rummage about until you find
the language you're looking for.  Note that many of the languages have
multiple variants.

There is also a default table that can be used for languages with no
tailorings.

[note Collation tailoring is quite expensive for some languages, typically the
CJK (Chinese, Japanese, and Korean) language tailorings _emdash_ sometimes as
much as multiple seconds.  There is serialization of collation tables to/from
a buffer or to/from a `boost::filesystem::path`.  This enables you to do
expensive tailorings offline, and just load the results at runtime.  See the
headers _table_ser_ and _save_load_table_ for details.]

[heading The Collation API]

The collation-related functions all require a collation table.  There are two
ways to collate two _cp_ sequences relative to one another.  First is to just
call `collate()`.  This can be very expensive to call in a loop or other hot
code path, because the sort keys are not kept for re-use.  The other way is to
create the sort keys for the sequences, and then compare the keys yourself.
Here is a simple example using the default table:

[collation_simple]

Here's a similar example, this time using the Danish collation tailorings.
Note that we're using two overloads that respectively take _CPRngs_ and
_GrRngs_.

[collation_simple_tailored]

I've made every effort to hide the messy details of collation configuration
from you, because they are super confusing.  Instead, _Text_ uses a set of
_coll_flags_ that have more recognizable semantics.  The flags map directly
onto the low-level configuration settings.  If you have C++14 _ce_ support,
combinations of the flags that map to incompatible low-level settings will not
compile.  The low-level configuration is still available for those already
familiar with Unicode collation.

When generating a sort key, the default configuration is to use tertiary
strength (consider everything but punctuation), with no other options enabled.
Other options can be specified to get different collations with the same
table:

[collation_configuration]

[important Unicode collation requires the NFD normalization form, one of the
least compact normalization forms. However, there is a variant that is defined
in _tn5_ that allows one to use an alternate normalization form called FCC.

FCC is very similar to NFC, except that it is less compact in a few
cases. _Text_'s collation implementation relies on the inputs being in the FCC
normalization form, so the collation functions require inputs to be normalized
FCC.  This happens automatically within the text layer types (_t_, _r_,
etc.).]

[heading Associative Container Woes]

One thing people use associative containers for is to make it easy to look for
elements, while keeping the elements sorted.  Let's see how that goes in the
world of Unicode:

[collation_set_1]

Hm.  Not well.  Ok, we can fix this by making a comparison callable:

[collation_text_cmp_naive]

Then we can make a set using that:

[collation_set_2]

But wait, I want to do a bunch of stuff in Danish.  In fact, I'm building a
directory of old and new town names, and I want to print an index of them for
end-users.  Ah, I'll just collate the values instead of binary-comparing them.

First, my new callable:

[collation_text_coll_cmp]

And then the using code:

[collation_set_3]

The problem is that we're doing the very expensive operation of creating two
sort keys for *each comparison*, and immediately throwing them away.

[note Ordering sequences of _cps_ creates expectations about their order that
can be misleading.  _Text_ therefore does not allow implicit ordering of
sequences of _cps_.]

Let's look at how much overhead we've incurred by doing all this repetitive
collation.  Below is the result of running a pair of perf tests.  The first
test is insertion of `N` _cp_ sequences into a `flat_multiset` that just does
binary comparison, like `set2` in the example above.  The second inserts the
same `N` sequences into a `flat_multiset` that does collation and throws away
the keys, like `set3` in the other example above.

[/ Before you can generate the tables below:
   perf/text_set_compare_vs_collate_perf --benchmark_format=json > DATA
/]

[/ ./make_perf_table.py DATA BM_set_inserts_binary_compare_text_naive,CodePointBinaryComparison BM_set_inserts_collate,FullCollation /]
[table Code Point Binary Comparison Vs. Unretained Key Collation
    [ [N] [Code Point Binary Comparison] [Full Collation] [Code Point Binary Comparison / Full Collation] ]
    [ [16] [5963 ns] [24777 ns] [0.241] ]
    [ [64] [52369 ns] [210472 ns] [0.249] ]
    [ [512] [1009486 ns] [3777195 ns] [0.267] ]
    [ [4096] [27348577 ns] [64371370 ns] [0.425] ]
    [ [8192] [51425527 ns] [133293696 ns] [0.386] ]
]

So the collation is truly a large cost!  But wait, isn't all that transcoding
from UTF-8 to UTF-32 taking up a lot of time?  We can do better that the code point binary comparison above by doing `char` binary comparison instead:

[collation_text_cmp]

[collation_set_4]

And here are the two binary comparison approaches compared to each other:

[/ ./make_perf_table.py DATA BM_set_inserts_binary_compare_text_naive,CodePointBinaryComparison BM_set_inserts_binary_compare_text,CharBinaryComparison /]
[table Code Point Binary Comparison Vs. `char` Binary Comparison (Tree Set)
    [ [N] [Code Point Binary Comparison] [`char` Binary Comparison] [Code Point Binary Comparison / `char` Binary Comparison] ]
    [ [16] [5963 ns] [5341 ns] [1.12] ]
    [ [64] [52369 ns] [52008 ns] [1.01] ]
    [ [512] [1009486 ns] [917452 ns] [1.1] ]
    [ [4096] [27348577 ns] [26544606 ns] [1.03] ]
    [ [8192] [51425527 ns] [49242103 ns] [1.04] ]
]

Interestingly, getting rid of UTF-8 -> UTF-32 transcoding only amounts to a
few percent.

A better way to go is probably to directly associate the sort key with each
sequence in a map:

[collation_map]

Here we compare the map-based approach to the faster binary comparison
approach from before:

[/ ./make_perf_table.py DATA BM_set_inserts_binary_compare_text,Set BM_map_inserts,Map /]
[table `char` Binary Comparison Flat Set Vs. Flat Map
    [ [N] [Set] [Map] [Set / Map] ]
    [ [16] [5341 ns] [5579 ns] [0.957] ]
    [ [64] [52008 ns] [30262 ns] [1.72] ]
    [ [512] [917452 ns] [511895 ns] [1.79] ]
    [ [4096] [26544606 ns] [22397834 ns] [1.19] ]
    [ [8192] [49242103 ns] [37479917 ns] [1.31] ]
]

The map version even does better than the set that uses binary comparison.

In all these tests, I've intentionally chosen to use multisets and multimaps,
because otherwise the map-based approach above has an unfair advantage, or a
bug, depending on your use case.

If you use a map instead of a multimap, you may not insert a value that has
the same sort key as an element already in the map, even if that _cp_ sequence
itself is *not* in the map.  That is, two _cp_ sequences can collate to the
same sort key.  If you want to consider two _cp_ sequences equivalent if they
collate the same, it is a feature, and a possibly large optimization, not to
insert one of them.  If you want to keep all unique _cp_ sequences, this is a
bug.

[heading Associative Container Perf Bottom Line]

As with everything else associated with Unicode, it's complicated:

* When using sequences of _cps_ with associative containers, consider using
  binary comparison if you *don't* care about the sort order being
  collation-correct.

* If you do binary comparisons of UTF-8 encoded types, consider comparing the
  bytes of the underlying `char`s directly, though it's not a large effect.

* You should use a container that retains the sort keys once they are
  generated if you *do* care about the sort order being collation-correct.

[heading Hashing Containers]

Associative containers are not the best choice when the key-type is a sequence
of things, because comparing sequences is inherently expensive.  Why not use
the unordered associative containers then?

[collation_unordered_map]

Hashing is your friend.  Hashing containers sidestep the question of
misleading orderings ("Is this order collation-correct?") completely, because
they are unordered.

[heading Sorting Woes]

TODO: collation keys should be kept around since they are expensive to make



TODO

[endsect]


[section Searching]

TODO

[endsect]


[section Bidirectional Text]

TODO

[endsect]

[endsect]
